{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5e25b6-fc45-4788-9e40-702c93ff4a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install required packages (run once per env)\n",
    "\n",
    "!pip -q install tweepy python-dotenv pandas vaderSentiment matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8cd4a01-40d2-4fc5-b4f1-ea47c2d1138f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import math\n",
    "import time\n",
    "import unicodedata\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from dotenv import load_dotenv\n",
    "import tweepy\n",
    "\n",
    "# Load env\n",
    "load_dotenv()\n",
    "BEARER = os.getenv(\"X_BEARER_TOKEN\")\n",
    "if not BEARER:\n",
    "    raise RuntimeError(\"X_BEARER_TOKEN not found. Please create a .env file with X_BEARER_TOKEN.\")\n",
    "client = tweepy.Client(bearer_token=BEARER, wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15819e10-ae91-47a6-a3d2-023694b7ba82",
   "metadata": {},
   "source": [
    "# Define search queries\n",
    "We focus on Australian politics. Note the Australian Labor Party is spelled **Labor** (not Labour). You can tweak '''MAX_TWEET_PER_QUERY''', date window, and the keyword lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458c4fa0-20a7-4f30-86bd-a529fea24c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query config\n",
    "MAX_TWEETS_PER_QUERY = 100 # Adjust based on your API tier/limits\n",
    "DAYS_BACK = 30 # Adjust based on period of time, here we are analysing sentiment in the past month\n",
    "LANG = \"en\" # English only\n",
    "\n",
    "SINCE = (dt.datetime.utcnow() - dt.timedelta(days=DAYS_BACK)).strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "\n",
    "# Hastags and Aussie context terms\n",
    "\n",
    "AUS_CONTEXT = [\"auspol\", \"Australia\", \"Australian\", \"Canberra\", \"Parliament\"]\n",
    "\n",
    "# Party-focused keyword sets\n",
    "\n",
    "LABOR_TERMS = [\n",
    "    \"Australian Labor Party\", \"ALP\", \"Labor Australia\", \"Anthony Albanese\", \"Albo\", \"@AlboMP\", \"@AustralianLabor\"\n",
    "]\n",
    "\n",
    "LIBERAL_TERMS = [\n",
    "    \"Liberal Party of Australia\", \"Liberal Australia\", \"LNP\", \"Coalition Australia\", \"Susan Ley\"\n",
    "]\n",
    "\n",
    "def build_query(terms):\n",
    "    # Compose an OR list, exclude retweets, filter by language.\n",
    "    or_block = \" OR \".join([f'(\"{t}\")' if \" \" in t else t for t in terms + AUS_CONTEXT])\n",
    "    # Avoid very common noise words here; you can add -is:reply to limit to original posts\n",
    "    q = f\"({or_block}) lang:{LANG} -is:retweet\"\n",
    "    return q\n",
    "\n",
    "QUERIES = {\n",
    "    \"Labor\": build_query(LABOR_TERMS),\n",
    "    \"Liberal\": build_query(LIBERAL_TERMS),\n",
    "}\n",
    "\n",
    "QUERIES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1820f3-33d8-4982-bd1f-8a4cdced76c3",
   "metadata": {},
   "source": [
    "# Fetch recent post with expansions and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830e54e1-d700-4891-80c1-70bda00a13f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_recent_safe(query, max_results, since_iso, client):\n",
    "    results = []\n",
    "    per_call = 100 if max_results >= 100 else max_results\n",
    "    next_token = None\n",
    "    fetched = 0\n",
    "    \n",
    "    print(f\"ğŸ” Fetching: '{query}' (target: {max_results} tweets)\")\n",
    "    \n",
    "    while fetched < max_results:\n",
    "        try:\n",
    "            remaining = max_results - fetched\n",
    "            current_batch = min(per_call, remaining)\n",
    "            \n",
    "            print(f\"â³ Progress: {fetched}/{max_results} tweets\", end=\"\\r\")\n",
    "            \n",
    "            resp = client.search_recent_tweets(\n",
    "                query=query,\n",
    "                max_results=current_batch,\n",
    "                start_time=since_iso,\n",
    "                tweet_fields=[\"id\",\"text\",\"lang\",\"created_at\",\"public_metrics\",\"possibly_sensitive\",\"source\"],\n",
    "                user_fields=[\"username\",\"name\",\"public_metrics\",\"verified\"],\n",
    "                expansions=[\"author_id\"],\n",
    "                next_token=next_token\n",
    "            )\n",
    "            \n",
    "            if resp.data:\n",
    "                # Process tweets (simplified version of your existing logic)\n",
    "                tweets = []\n",
    "                users_dict = {}\n",
    "                \n",
    "                if resp.includes and 'users' in resp.includes:\n",
    "                    users_dict = {user.id: user for user in resp.includes['users']}\n",
    "                \n",
    "                for tweet in resp.data:\n",
    "                    tweet_data = {\n",
    "                        'id': tweet.id,\n",
    "                        'text': tweet.text,\n",
    "                        'created_at': tweet.created_at.isoformat() if tweet.created_at else None,\n",
    "                        'lang': tweet.lang,\n",
    "                        'public_metrics': tweet.public_metrics,\n",
    "                        'author_id': tweet.author_id\n",
    "                    }\n",
    "                    \n",
    "                    if tweet.author_id in users_dict:\n",
    "                        user = users_dict[tweet.author_id]\n",
    "                        tweet_data.update({\n",
    "                            'username': user.username,\n",
    "                            'name': user.name,\n",
    "                            'verified': user.verified\n",
    "                        })\n",
    "                    \n",
    "                    tweets.append(tweet_data)\n",
    "                \n",
    "                results.extend(tweets)\n",
    "                fetched += len(tweets)\n",
    "                \n",
    "                if hasattr(resp, 'meta') and 'next_token' in resp.meta:\n",
    "                    next_token = resp.meta['next_token']\n",
    "                else:\n",
    "                    break\n",
    "            else:\n",
    "                break\n",
    "                \n",
    "        except tweepy.TooManyRequests:\n",
    "            print(f\"\\nâš ï¸  Rate limited! Waiting 60s... (Press Ctrl+C to interrupt)\")\n",
    "            try:\n",
    "                # Interruptible countdown\n",
    "                for i in range(60, 0, -1):\n",
    "                    print(f\"\\râ³ {i:2d}s remaining\", end=\"\", flush=True)\n",
    "                    time.sleep(1)\n",
    "                print(f\"\\râœ… Resuming...           \")\n",
    "            except KeyboardInterrupt:\n",
    "                print(f\"\\nğŸ›‘ Interrupted during rate limit wait. Returning {len(results)} tweets.\")\n",
    "                return results\n",
    "                \n",
    "        except KeyboardInterrupt:\n",
    "            print(f\"\\nğŸ›‘ Fetch interrupted. Returning {len(results)} tweets collected so far.\")\n",
    "            return results\n",
    "            \n",
    "    print(f\"\\nâœ… Complete: {len(results)} tweets collected\")\n",
    "    return results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
