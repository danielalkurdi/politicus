{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15819e10-ae91-47a6-a3d2-023694b7ba82",
   "metadata": {},
   "source": [
    "# Australian Political Sentiment Analysis Configuration\n",
    "\n",
    "This notebook provides a comprehensive sentiment analysis system for Australian political discourse on X (formerly Twitter).\n",
    "\n",
    "## üìä **Configuration Options**\n",
    "You can easily customize the analysis by modifying the `AnalysisConfig` class parameters:\n",
    "\n",
    "- `MAX_TWEETS_PER_QUERY`: Number of tweets to collect per party (default: 100)\n",
    "- `DAYS_BACK`: Analysis period in days (default: 30)\n",
    "- `LANG`: Tweet language filter (default: \"en\")\n",
    "- `API_BATCH_SIZE`: API request batch size (default: 100)\n",
    "- `MIN_TEXT_LENGTH`: Minimum tweet length filter (default: 10)\n",
    "\n",
    "## üèõÔ∏è **Political Party Focus**\n",
    "The system focuses on Australian federal politics with comprehensive keyword sets:\n",
    "\n",
    "**Labor Party**: Australian Labor Party is spelled **\"Labor\"** (not \"Labour\"). Includes:\n",
    "- Official party names, handles, and variations\n",
    "- Current leadership: Anthony Albanese, Richard Marles, Jim Chalmers, etc.\n",
    "- State branches: NSW Labor, Victorian Labor, QLD Labor, etc.\n",
    "\n",
    "**Liberal/Coalition**: Includes Liberal Party and Coalition partners:\n",
    "- Party names: Liberal Party of Australia, LNP, Coalition\n",
    "- Current leadership: Sussan Ley, Peter Dutton, Angus Taylor, etc.\n",
    "- Coalition partners: The Nationals, LNP Queensland\n",
    "\n",
    "## üöÄ **Architecture Overview**\n",
    "- `TwitterDataFetcher`: API calls with rate limiting and error handling\n",
    "- `DataCollector`: Multi-party data collection orchestration\n",
    "- `SentimentAnalyzer`: VADER-based sentiment analysis with comprehensive metrics\n",
    "- `ReportGenerator`: Professional visualizations and reporting\n",
    "- `AnalysisOrchestrator`: Main pipeline coordination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5e25b6-fc45-4788-9e40-702c93ff4a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install required packages (run once per env)\n",
    "\n",
    "!pip -q install tweepy python-dotenv pandas vaderSentiment matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8cd4a01-40d2-4fc5-b4f1-ea47c2d1138f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import math\n",
    "import time\n",
    "import unicodedata\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from dotenv import load_dotenv\n",
    "import tweepy\n",
    "\n",
    "# Load env\n",
    "load_dotenv()\n",
    "BEARER = os.getenv(\"X_BEARER_TOKEN\")\n",
    "if not BEARER:\n",
    "    raise RuntimeError(\"X_BEARER_TOKEN not found. Please create a .env file with X_BEARER_TOKEN.\")\n",
    "client = tweepy.Client(bearer_token=BEARER, wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458c4fa0-20a7-4f30-86bd-a529fea24c43",
   "metadata": {},
   "outputs": [],
   "source": "# Configuration and Custom Exceptions\nfrom dataclasses import dataclass\nfrom typing import List, Dict, Any, Optional\nimport logging\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass AnalysisConfig:\n    \"\"\"Configuration class for political sentiment analysis\"\"\"\n    MAX_TWEETS_PER_QUERY: int = 10  # Reduced from 100 - Free tier has severe limits\n    DAYS_BACK: int = 6  # Reduced from 7 to avoid edge cases with API timing\n    LANG: str = \"en\"\n    RATE_LIMIT_WAIT: int = 900  # 15 minutes wait for Free tier\n    API_BATCH_SIZE: int = 10  # Much smaller batches for Free tier\n    MIN_TEXT_LENGTH: int = 10\n    INTER_QUERY_DELAY: int = 960  # 16 minutes between different party queries for Free tier\n    \n    # Australian political context terms\n    AUS_CONTEXT: Optional[List[str]] = None\n    LABOR_TERMS: Optional[List[str]] = None\n    LIBERAL_TERMS: Optional[List[str]] = None\n    \n    def __post_init__(self):\n        # Broad Australian-politics context seen on X\n        if self.AUS_CONTEXT is None:\n            self.AUS_CONTEXT = [\n                # high-signal hashtags/terms - reduced for Free tier efficiency\n                \"auspol\", \"AusPol\", \"auspolitics\",\n                \"Australian politics\", \"Australian Government\",\n                \"Prime Minister\", \"Opposition\"\n            ]\n\n        # Australian Labor Party (federal) ‚Äî focused on highest-signal terms for Free tier\n        if self.LABOR_TERMS is None:\n            self.LABOR_TERMS = [\n                # Most important party identifiers\n                \"Australian Labor Party\", \"ALP\", \"Labor Australia\",\n                \"@AustralianLabor\",\n                # Key figures most likely to generate discussion\n                \"Anthony Albanese\", \"Albo\", \"@AlboMP\",\n                \"Jim Chalmers\", \"@JEChalmers\",\n                \"Penny Wong\", \"@SenatorWong\"\n            ]\n\n        # Liberal / Coalition context - focused on highest-signal terms\n        if self.LIBERAL_TERMS is None:\n            self.LIBERAL_TERMS = [\n                # Most important party identifiers\n                \"Liberal Party of Australia\", \"Liberal Australia\", \"Coalition\",\n                \"@LiberalAus\",\n                # Key figures most likely to generate discussion  \n                \"Peter Dutton\", \"@PeterDutton_MP\",\n                \"Sussan Ley\", \"@sussanley\",\n                \"Angus Taylor\", \"@AngusTaylorMP\"\n            ]\n    \n    @property\n    def since_date(self) -> str:\n        \"\"\"Calculate the start date for tweet collection\"\"\"\n        # Use timezone-aware datetime and ensure we don't exceed Twitter's 7-day limit\n        import datetime\n        max_days_back = min(self.DAYS_BACK, 6)  # Conservative limit for API stability\n        start_time = datetime.datetime.now(datetime.timezone.utc) - datetime.timedelta(days=max_days_back)\n        formatted_time = start_time.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n        logger.info(f\"Using start_time: {formatted_time} ({max_days_back} days back)\")\n        logger.warning(f\"üö® FREE TIER DETECTED: Only 1 API request per 15 minutes allowed!\")\n        logger.info(f\"üí° Consider upgrading to Basic Tier ($200/month) for 60 requests per 15 minutes\")\n        return formatted_time\n\n# Custom Exception Classes\nclass TwitterAPIError(Exception):\n    \"\"\"Raised when Twitter API encounters an error\"\"\"\n    pass\n\nclass RateLimitError(Exception):\n    \"\"\"Raised when rate limit is exceeded\"\"\"\n    pass\n\nclass DataProcessingError(Exception):\n    \"\"\"Raised when data processing fails\"\"\"\n    pass\n\nclass SentimentAnalysisError(Exception):\n    \"\"\"Raised when sentiment analysis fails\"\"\"\n    pass\n\n# Initialize configuration\nconfig = AnalysisConfig()\n\ndef build_query(terms: List[str], config: AnalysisConfig) -> str:\n    \"\"\"Build search query from terms and context - optimized for Free tier\"\"\"\n    try:\n        # Ensure we have non-null terms and context\n        if not terms or not config.AUS_CONTEXT:\n            raise DataProcessingError(\"Terms or AUS_CONTEXT cannot be empty\")\n        \n        # For Free tier, use fewer terms to avoid complex queries\n        combined_terms = terms + config.AUS_CONTEXT\n        # Limit to most important terms for Free tier efficiency\n        top_terms = combined_terms[:8]  # Reduced from all terms\n        \n        or_block = \" OR \".join([f'\"{t}\"' if \" \" in t else t for t in top_terms])\n        query = f\"({or_block}) lang:{config.LANG} -is:retweet\"\n        logger.info(f\"Built Free-tier optimized query: {query[:100]}...\")\n        return query\n    except Exception as e:\n        logger.error(f\"Error building query: {e}\")\n        raise DataProcessingError(f\"Failed to build query: {e}\")\n\n# Build queries using configuration - with Free tier warnings\nQUERIES = {\n    \"Labor\": build_query(config.LABOR_TERMS or [], config),\n    \"Liberal\": build_query(config.LIBERAL_TERMS or [], config),\n}\n\nlogger.warning(\"üö® X API FREE TIER LIMITATIONS:\")\nlogger.warning(\"   ‚Ä¢ Only 1 request per 15 minutes\")\nlogger.warning(\"   ‚Ä¢ 100 tweet cap per month\")  \nlogger.warning(\"   ‚Ä¢ Analysis will take ~32 minutes for both parties\")\nlogger.warning(\"üí° For production use, consider Basic Tier ($200/month)\")\nlogger.info(f\"Configuration initialized: {config.MAX_TWEETS_PER_QUERY} tweets per query, {config.DAYS_BACK} days back\")\n\nQUERIES"
  },
  {
   "cell_type": "markdown",
   "id": "5e1820f3-33d8-4982-bd1f-8a4cdced76c3",
   "metadata": {},
   "source": [
    "# Fetch recent post with expansions and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830e54e1-d700-4891-80c1-70bda00a13f4",
   "metadata": {},
   "outputs": [],
   "source": "class TwitterDataFetcher:\n    \"\"\"Handles Twitter data fetching with rate limiting and error handling\"\"\"\n    \n    def __init__(self, client, config: AnalysisConfig):\n        self.client = client\n        self.config = config\n        self.logger = logging.getLogger(__name__)\n    \n    def _handle_rate_limit(self) -> bool:\n        \"\"\"Handle rate limiting with interruptible wait\"\"\"\n        self.logger.warning(\"Rate limited! Waiting 60s...\")\n        try:\n            for i in range(self.config.RATE_LIMIT_WAIT, 0, -1):\n                print(f\"\\r‚è≥ {i:2d}s remaining\", end=\"\", flush=True)\n                time.sleep(1)\n            print(\"\\r‚úÖ Resuming...           \")\n            return True\n        except KeyboardInterrupt:\n            self.logger.info(\"Interrupted during rate limit wait\")\n            return False\n    \n    def _process_tweet_batch(self, resp) -> List[Dict[str, Any]]:\n        \"\"\"Process a batch of tweets from API response\"\"\"\n        if not resp or not resp.data:\n            return []\n        \n        tweets = []\n        users_dict = {}\n        \n        # Build user lookup dictionary\n        if resp.includes and 'users' in resp.includes:\n            users_dict = {user.id: user for user in resp.includes['users']}\n        \n        # Process each tweet\n        for tweet in resp.data:\n            tweet_data = {\n                'id': tweet.id,\n                'text': tweet.text,\n                'created_at': tweet.created_at.isoformat() if tweet.created_at else None,\n                'lang': tweet.lang,\n                'public_metrics': tweet.public_metrics,\n                'author_id': tweet.author_id\n            }\n            \n            # Add user data if available\n            if tweet.author_id in users_dict:\n                user = users_dict[tweet.author_id]\n                tweet_data.update({\n                    'username': user.username,\n                    'name': user.name,\n                    'verified': user.verified\n                })\n            \n            tweets.append(tweet_data)\n        \n        return tweets\n    \n    def _make_api_call(self, query: str, batch_size: int, since_iso: str, next_token: Optional[str] = None):\n        \"\"\"Make a single API call with proper error handling\"\"\"\n        try:\n            kwargs = {\n                'query': query,\n                'max_results': batch_size,\n                'start_time': since_iso,\n                'tweet_fields': [\"id\", \"text\", \"lang\", \"created_at\", \"public_metrics\", \"possibly_sensitive\", \"source\"],\n                'user_fields': [\"username\", \"name\", \"public_metrics\", \"verified\"],\n                'expansions': [\"author_id\"]\n            }\n            \n            # Only add next_token if it's not None\n            if next_token is not None:\n                kwargs['next_token'] = next_token\n            \n            return self.client.search_recent_tweets(**kwargs)\n            \n        except tweepy.TooManyRequests as e:\n            raise RateLimitError(\"API rate limit exceeded\")\n        except tweepy.TwitterServerError as e:\n            raise TwitterAPIError(f\"Twitter server error: {e}\")\n        except Exception as e:\n            raise TwitterAPIError(f\"API call failed: {e}\")\n    \n    def fetch_recent_safe(self, query: str, max_results: int, since_iso: str) -> List[Dict[str, Any]]:\n        \"\"\"Fetch recent tweets with safe error handling and progress tracking\"\"\"\n        results = []\n        per_call = min(self.config.API_BATCH_SIZE, max_results)\n        next_token: Optional[str] = None\n        fetched = 0\n        \n        self.logger.info(f\"Fetching: '{query[:50]}...' (target: {max_results} tweets)\")\n        \n        try:\n            while fetched < max_results:\n                remaining = max_results - fetched\n                current_batch = min(per_call, remaining)\n                \n                print(f\"‚è≥ Progress: {fetched}/{max_results} tweets\", end=\"\\r\")\n                \n                try:\n                    resp = self._make_api_call(query, current_batch, since_iso, next_token)\n                    \n                    if resp and resp.data:\n                        tweets = self._process_tweet_batch(resp)\n                        results.extend(tweets)\n                        fetched += len(tweets)\n                        \n                        # Check for next token - handle both None and missing attribute cases\n                        if hasattr(resp, 'meta') and resp.meta and 'next_token' in resp.meta:\n                            next_token = resp.meta['next_token']\n                        else:\n                            next_token = None\n                            self.logger.info(\"No more tweets available\")\n                            break\n                    else:\n                        self.logger.info(\"No data returned from API\")\n                        break\n                \n                except RateLimitError:\n                    if not self._handle_rate_limit():\n                        break\n                \n                except TwitterAPIError as e:\n                    self.logger.error(f\"API error: {e}\")\n                    break\n                    \n        except KeyboardInterrupt:\n            self.logger.info(f\"Fetch interrupted. Returning {len(results)} tweets collected so far\")\n        \n        print(f\"\\n‚úÖ Complete: {len(results)} tweets collected\")\n        self.logger.info(f\"Successfully fetched {len(results)} tweets\")\n        return results\n\ndef fetch_recent_safe(query, max_results, since_iso, client):\n    \"\"\"Legacy wrapper function for backward compatibility\"\"\"\n    fetcher = TwitterDataFetcher(client, config)\n    return fetcher.fetch_recent_safe(query, max_results, since_iso)"
  },
  {
   "cell_type": "markdown",
   "id": "k2slvlz0ywc",
   "metadata": {},
   "source": [
    "# Data Processing & Analysis Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "x35lpvkjhwp",
   "metadata": {},
   "outputs": [],
   "source": "class DataCollector:\n    \"\"\"Handles collection and organization of Twitter data\"\"\"\n    \n    def __init__(self, client, config: AnalysisConfig):\n        self.client = client\n        self.config = config\n        self.fetcher = TwitterDataFetcher(client, config)\n        self.logger = logging.getLogger(__name__)\n    \n    def collect_all_data(self) -> Dict[str, pd.DataFrame]:\n        \"\"\"Collect tweets for all political parties - optimized for Free tier\"\"\"\n        all_data = {}\n        \n        self.logger.warning(\"üö® FREE TIER: Each party query requires 15+ minute wait\")\n        self.logger.info(\"üí° For faster analysis, consider X API Basic Tier ($200/month)\")\n        \n        for i, (party, query) in enumerate(QUERIES.items()):\n            self.logger.info(f\"Collecting data for {party}...\")\n            print(f\"\\nüìä Collecting data for {party}...\")\n            \n            if i > 0:  # Add delay between different party queries for Free tier\n                wait_time = self.config.INTER_QUERY_DELAY\n                self.logger.info(f\"‚è≥ FREE TIER: Waiting {wait_time//60} minutes before next party query...\")\n                print(f\"‚è≥ FREE TIER: Waiting {wait_time//60} minutes before next party query...\")\n                \n                try:\n                    for remaining in range(wait_time, 0, -30):  # Update every 30 seconds\n                        mins, secs = divmod(remaining, 60)\n                        print(f\"\\r‚è≥ Wait time remaining: {mins:02d}:{secs:02d}\", end=\"\", flush=True)\n                        time.sleep(30)\n                    print(\"\\n‚úÖ Wait complete, proceeding with next query...\")\n                except KeyboardInterrupt:\n                    self.logger.info(\"Wait interrupted by user\")\n                    print(\"\\nüõë Wait interrupted by user\")\n                    break\n            \n            try:\n                tweets = self.fetcher.fetch_recent_safe(\n                    query, \n                    self.config.MAX_TWEETS_PER_QUERY, \n                    self.config.since_date\n                )\n                \n                # Convert to DataFrame\n                df = pd.DataFrame(tweets)\n                if not df.empty and 'public_metrics' in df.columns:\n                    # Flatten public metrics - convert Series to list first\n                    metrics_list = df['public_metrics'].tolist()\n                    \n                    # Only normalize if we have valid data\n                    if metrics_list and all(isinstance(item, dict) for item in metrics_list if item is not None):\n                        try:\n                            metrics_df = pd.json_normalize(metrics_list)\n                            df = pd.concat([df.drop('public_metrics', axis=1), metrics_df], axis=1)\n                        except Exception as e:\n                            self.logger.warning(f\"Could not normalize public_metrics for {party}: {e}\")\n                    \n                    df['party_query'] = party\n                elif not df.empty:\n                    # If no public_metrics column, just add party_query\n                    df['party_query'] = party\n                \n                all_data[party] = df\n                self.logger.info(f\"‚úÖ {party}: {len(df)} tweets collected\")\n                print(f\"‚úÖ {party}: {len(df)} tweets collected\")\n                \n                # No additional pause needed here - the fetcher handles rate limiting\n                \n            except Exception as e:\n                self.logger.error(f\"Error collecting data for {party}: {e}\")\n                all_data[party] = pd.DataFrame()\n        \n        total_tweets = sum(len(df) for df in all_data.values())\n        self.logger.info(f\"üèÅ Collection complete: {total_tweets} total tweets from all parties\")\n        \n        if total_tweets == 0:\n            self.logger.warning(\"‚ö†Ô∏è No tweets collected. This could be due to:\")\n            self.logger.warning(\"   ‚Ä¢ Free tier API limits (only 1 request per 15 minutes)\")\n            self.logger.warning(\"   ‚Ä¢ Low political activity in the past 6 days\")\n            self.logger.warning(\"   ‚Ä¢ Query terms not matching recent tweets\")\n            self.logger.warning(\"üí° Try running again later or consider API upgrade\")\n        \n        return all_data\n\nclass DataProcessor:\n    \"\"\"Handles data cleaning and preprocessing\"\"\"\n    \n    def __init__(self, config: AnalysisConfig):\n        self.config = config\n        self.logger = logging.getLogger(__name__)\n    \n    def clean_text(self, text: str) -> str:\n        \"\"\"Clean tweet text for sentiment analysis\"\"\"\n        if pd.isna(text) or not text:\n            return \"\"\n        \n        try:\n            # Remove URLs\n            text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n            \n            # Remove user mentions and hashtags (keep the text)\n            text = re.sub(r'@\\w+', '', text)\n            text = re.sub(r'#(\\w+)', r'\\1', text)\n            \n            # Normalize unicode characters\n            text = unicodedata.normalize('NFKD', text)\n            \n            # Remove extra whitespace\n            text = ' '.join(text.split())\n            \n            return text.strip()\n        \n        except Exception as e:\n            self.logger.error(f\"Error cleaning text: {e}\")\n            return \"\"\n    \n    def classify_party_sentiment(self, text: str) -> str:\n        \"\"\"Classify which party a tweet is more likely about\"\"\"\n        if not text:\n            return 'Neutral'\n        \n        try:\n            text_lower = text.lower()\n            \n            labor_score = sum(\n                1 for term in self.config.LABOR_TERMS \n                if term.lower().replace('@', '') in text_lower\n            )\n            liberal_score = sum(\n                1 for term in self.config.LIBERAL_TERMS \n                if term.lower().replace('@', '') in text_lower\n            )\n            \n            if labor_score > liberal_score:\n                return 'Labor'\n            elif liberal_score > labor_score:\n                return 'Liberal'\n            else:\n                return 'Neutral'\n        \n        except Exception as e:\n            self.logger.error(f\"Error classifying party sentiment: {e}\")\n            return 'Neutral'\n    \n    def validate_data(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Validate and filter data quality\"\"\"\n        if df.empty:\n            return df\n        \n        original_count = len(df)\n        \n        # Remove tweets that are too short\n        if 'text' in df.columns:\n            df = df[df['text'].str.len() >= self.config.MIN_TEXT_LENGTH]\n        \n        # Remove duplicates\n        if 'id' in df.columns:\n            df = df.drop_duplicates(subset=['id'])\n        \n        self.logger.info(f\"Data validation: {original_count} -> {len(df)} tweets\")\n        return df\n\n# Legacy wrapper functions for backward compatibility\ndef collect_all_data():\n    \"\"\"Legacy wrapper function\"\"\"\n    collector = DataCollector(client, config)\n    return collector.collect_all_data()\n\ndef clean_text(text):\n    \"\"\"Legacy wrapper function\"\"\"\n    processor = DataProcessor(config)\n    return processor.clean_text(text)\n\ndef classify_party_sentiment(text, labor_terms, liberal_terms):\n    \"\"\"Legacy wrapper function\"\"\"\n    processor = DataProcessor(config)\n    return processor.classify_party_sentiment(text)"
  },
  {
   "cell_type": "markdown",
   "id": "1ks2gfc7hmt",
   "metadata": {},
   "source": [
    "# Sentiment Analysis with VADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nc9qotuogo",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentAnalyzer:\n",
    "    \"\"\"Handles sentiment analysis using VADER sentiment analyzer\"\"\"\n",
    "    \n",
    "    def __init__(self, config: AnalysisConfig):\n",
    "        self.config = config\n",
    "        self.analyzer = SentimentIntensityAnalyzer()\n",
    "        self.processor = DataProcessor(config)\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "    \n",
    "    def _classify_sentiment(self, compound_score: float) -> str:\n",
    "        \"\"\"Classify sentiment based on compound score\"\"\"\n",
    "        if compound_score >= 0.05:\n",
    "            return 'Positive'\n",
    "        elif compound_score <= -0.05:\n",
    "            return 'Negative'\n",
    "        else:\n",
    "            return 'Neutral'\n",
    "    \n",
    "    def analyze_sentiment_batch(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Perform sentiment analysis on a DataFrame of tweets\"\"\"\n",
    "        if df.empty:\n",
    "            self.logger.warning(\"Empty DataFrame provided for sentiment analysis\")\n",
    "            return df\n",
    "        \n",
    "        try:\n",
    "            # Validate and clean data first\n",
    "            df = self.processor.validate_data(df.copy())\n",
    "            \n",
    "            if df.empty:\n",
    "                self.logger.warning(\"No valid data after validation\")\n",
    "                return df\n",
    "            \n",
    "            # Clean text\n",
    "            df['cleaned_text'] = df['text'].apply(self.processor.clean_text)\n",
    "            \n",
    "            # Apply VADER sentiment analysis\n",
    "            sentiment_scores = df['cleaned_text'].apply(\n",
    "                lambda x: self.analyzer.polarity_scores(x) if x else {'compound': 0, 'pos': 0, 'neg': 0, 'neu': 1}\n",
    "            )\n",
    "            \n",
    "            # Extract sentiment components\n",
    "            df['sentiment_compound'] = sentiment_scores.apply(lambda x: x['compound'])\n",
    "            df['sentiment_positive'] = sentiment_scores.apply(lambda x: x['pos'])\n",
    "            df['sentiment_negative'] = sentiment_scores.apply(lambda x: x['neg'])\n",
    "            df['sentiment_neutral'] = sentiment_scores.apply(lambda x: x['neu'])\n",
    "            \n",
    "            # Classify overall sentiment\n",
    "            df['sentiment_label'] = df['sentiment_compound'].apply(self._classify_sentiment)\n",
    "            \n",
    "            # Classify party relevance\n",
    "            df['party_classification'] = df['text'].apply(self.processor.classify_party_sentiment)\n",
    "            \n",
    "            self.logger.info(f\"Sentiment analysis completed for {len(df)} tweets\")\n",
    "            return df\n",
    "        \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error in sentiment analysis: {e}\")\n",
    "            raise SentimentAnalysisError(f\"Sentiment analysis failed: {e}\")\n",
    "    \n",
    "    def get_sentiment_summary(self, df: pd.DataFrame, party_name: str) -> Dict[str, Any]:\n",
    "        \"\"\"Generate comprehensive sentiment summary statistics\"\"\"\n",
    "        if df.empty:\n",
    "            return {'party': party_name, 'error': 'No data available'}\n",
    "        \n",
    "        try:\n",
    "            summary = {\n",
    "                'party': party_name,\n",
    "                'total_tweets': len(df),\n",
    "                'avg_compound_score': df['sentiment_compound'].mean() if 'sentiment_compound' in df.columns else 0,\n",
    "                'sentiment_distribution': {},\n",
    "                'party_classification': {},\n",
    "                'avg_engagement': {},\n",
    "                'quality_metrics': {\n",
    "                    'avg_text_length': df['text'].str.len().mean() if 'text' in df.columns else 0,\n",
    "                    'verified_users': df['verified'].sum() if 'verified' in df.columns else 0\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            # Sentiment distribution\n",
    "            if 'sentiment_label' in df.columns:\n",
    "                summary['sentiment_distribution'] = df['sentiment_label'].value_counts().to_dict()\n",
    "            \n",
    "            # Party classification\n",
    "            if 'party_classification' in df.columns:\n",
    "                summary['party_classification'] = df['party_classification'].value_counts().to_dict()\n",
    "            \n",
    "            # Engagement metrics\n",
    "            engagement_cols = ['like_count', 'retweet_count', 'reply_count', 'quote_count']\n",
    "            for col in engagement_cols:\n",
    "                if col in df.columns:\n",
    "                    summary['avg_engagement'][col] = df[col].mean()\n",
    "            \n",
    "            self.logger.info(f\"Summary generated for {party_name}: {summary['total_tweets']} tweets\")\n",
    "            return summary\n",
    "        \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error generating summary for {party_name}: {e}\")\n",
    "            return {'party': party_name, 'error': str(e)}\n",
    "\n",
    "class AnalysisOrchestrator:\n",
    "    \"\"\"Main orchestrator for the complete analysis pipeline\"\"\"\n",
    "    \n",
    "    def __init__(self, client, config: AnalysisConfig):\n",
    "        self.client = client\n",
    "        self.config = config\n",
    "        self.collector = DataCollector(client, config)\n",
    "        self.analyzer = SentimentAnalyzer(config)\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "    \n",
    "    def run_complete_analysis(self) -> tuple[Dict[str, pd.DataFrame], Dict[str, Dict[str, Any]]]:\n",
    "        \"\"\"Run the complete analysis pipeline\"\"\"\n",
    "        try:\n",
    "            self.logger.info(\"Starting complete analysis pipeline\")\n",
    "            \n",
    "            # Step 1: Collect data\n",
    "            self.logger.info(\"Step 1: Data collection\")\n",
    "            data_dict = self.collector.collect_all_data()\n",
    "            \n",
    "            # Check if we have any data\n",
    "            total_collected = sum(len(df) for df in data_dict.values())\n",
    "            if total_collected == 0:\n",
    "                raise DataProcessingError(\"No data collected. Check API credentials and network connection.\")\n",
    "            \n",
    "            self.logger.info(f\"Data collection complete! Total tweets: {total_collected}\")\n",
    "            \n",
    "            # Step 2: Analyze sentiment\n",
    "            self.logger.info(\"Step 2: Sentiment analysis\")\n",
    "            analyzed_data = {}\n",
    "            summaries = {}\n",
    "            \n",
    "            for party, df in data_dict.items():\n",
    "                if not df.empty:\n",
    "                    self.logger.info(f\"Analyzing sentiment for {party}...\")\n",
    "                    analyzed_df = self.analyzer.analyze_sentiment_batch(df)\n",
    "                    analyzed_data[party] = analyzed_df\n",
    "                    summaries[party] = self.analyzer.get_sentiment_summary(analyzed_df, party)\n",
    "                else:\n",
    "                    self.logger.warning(f\"No data for {party} - skipping\")\n",
    "                    analyzed_data[party] = df\n",
    "            \n",
    "            self.logger.info(\"Analysis pipeline completed successfully\")\n",
    "            return analyzed_data, summaries\n",
    "        \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error in analysis pipeline: {e}\")\n",
    "            raise\n",
    "\n",
    "# Legacy wrapper functions for backward compatibility\n",
    "def analyze_sentiment_batch(df):\n",
    "    \"\"\"Legacy wrapper function\"\"\"\n",
    "    analyzer = SentimentAnalyzer(config)\n",
    "    return analyzer.analyze_sentiment_batch(df)\n",
    "\n",
    "def get_sentiment_summary(df, party_name):\n",
    "    \"\"\"Legacy wrapper function\"\"\"\n",
    "    analyzer = SentimentAnalyzer(config)\n",
    "    return analyzer.get_sentiment_summary(df, party_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d0jkv8hc9",
   "metadata": {},
   "source": [
    "# Data Visualization & Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "niutzt77fz",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReportGenerator:\n",
    "    \"\"\"Handles visualization and report generation\"\"\"\n",
    "    \n",
    "    def __init__(self, config: AnalysisConfig):\n",
    "        self.config = config\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "    \n",
    "    def _plot_sentiment_distribution(self, ax, data_dict, parties, colors):\n",
    "        \"\"\"Create sentiment distribution comparison plot\"\"\"\n",
    "        sentiment_data = {}\n",
    "        for party in parties:\n",
    "            df = data_dict[party]\n",
    "            if not df.empty and 'sentiment_label' in df.columns:\n",
    "                sentiment_counts = df['sentiment_label'].value_counts()\n",
    "                sentiment_data[party] = sentiment_counts\n",
    "        \n",
    "        # Create grouped bar chart\n",
    "        sentiments = ['Positive', 'Negative', 'Neutral']\n",
    "        x_pos = range(len(sentiments))\n",
    "        width = 0.35\n",
    "        \n",
    "        for i, party in enumerate(parties):\n",
    "            if party in sentiment_data:\n",
    "                values = [sentiment_data[party].get(s, 0) for s in sentiments]\n",
    "                ax.bar([x + width*i for x in x_pos], values, width, \n",
    "                       label=party, color=colors[i % len(colors)], alpha=0.8)\n",
    "        \n",
    "        ax.set_xlabel('Sentiment')\n",
    "        ax.set_ylabel('Number of Tweets')\n",
    "        ax.set_title('Sentiment Distribution by Party')\n",
    "        ax.set_xticks([x + width/2 for x in x_pos])\n",
    "        ax.set_xticklabels(sentiments)\n",
    "        ax.legend()\n",
    "        ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    def _plot_average_sentiment(self, ax, data_dict, parties, colors):\n",
    "        \"\"\"Create average sentiment scores plot\"\"\"\n",
    "        avg_scores = []\n",
    "        party_names = []\n",
    "        for party in parties:\n",
    "            df = data_dict[party]\n",
    "            if not df.empty and 'sentiment_compound' in df.columns:\n",
    "                avg_scores.append(df['sentiment_compound'].mean())\n",
    "                party_names.append(party)\n",
    "        \n",
    "        if avg_scores:\n",
    "            bars = ax.bar(party_names, avg_scores, color=colors[:len(party_names)], alpha=0.8)\n",
    "            ax.set_ylabel('Average Sentiment Score')\n",
    "            ax.set_title('Average Sentiment Score by Party')\n",
    "            ax.set_ylim(-0.5, 0.5)\n",
    "            ax.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "            ax.grid(axis='y', alpha=0.3)\n",
    "            \n",
    "            # Add value labels on bars\n",
    "            for bar, score in zip(bars, avg_scores):\n",
    "                height = bar.get_height()\n",
    "                ax.text(bar.get_x() + bar.get_width()/2., height + 0.01 if height > 0 else height - 0.03,\n",
    "                        f'{score:.3f}', ha='center', va='bottom' if height > 0 else 'top')\n",
    "    \n",
    "    def _plot_engagement_metrics(self, ax, data_dict, parties, colors):\n",
    "        \"\"\"Create engagement metrics comparison plot\"\"\"\n",
    "        engagement_metrics = ['like_count', 'retweet_count', 'reply_count']\n",
    "        width = 0.35\n",
    "        \n",
    "        for i, party in enumerate(parties):\n",
    "            df = data_dict[party]\n",
    "            if not df.empty and all(col in df.columns for col in engagement_metrics):\n",
    "                values = [df[col].mean() for col in engagement_metrics]\n",
    "                ax.bar([x + width*i for x in range(len(engagement_metrics))], values, \n",
    "                       width, label=party, color=colors[i % len(colors)], alpha=0.8)\n",
    "        \n",
    "        ax.set_xlabel('Engagement Type')\n",
    "        ax.set_ylabel('Average Count')\n",
    "        ax.set_title('Average Engagement by Party')\n",
    "        ax.set_xticks([x + width/2 for x in range(len(engagement_metrics))])\n",
    "        ax.set_xticklabels(['Likes', 'Retweets', 'Replies'])\n",
    "        ax.legend()\n",
    "        ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    def _plot_party_classification(self, ax, data_dict, parties):\n",
    "        \"\"\"Create party classification distribution plot\"\"\"\n",
    "        classification_data = {}\n",
    "        for party in parties:\n",
    "            df = data_dict[party]\n",
    "            if not df.empty and 'party_classification' in df.columns:\n",
    "                class_counts = df['party_classification'].value_counts()\n",
    "                classification_data[party] = class_counts\n",
    "        \n",
    "        # Stacked bar chart\n",
    "        classifications = ['Labor', 'Liberal', 'Neutral']\n",
    "        \n",
    "        for i, class_type in enumerate(classifications):\n",
    "            values = []\n",
    "            for party in parties:\n",
    "                if party in classification_data:\n",
    "                    values.append(classification_data[party].get(class_type, 0))\n",
    "                else:\n",
    "                    values.append(0)\n",
    "            \n",
    "            bottom_values = [0] * len(parties)\n",
    "            for j in range(i):\n",
    "                prev_class = classifications[j]\n",
    "                for k, party in enumerate(parties):\n",
    "                    if party in classification_data:\n",
    "                        bottom_values[k] += classification_data[party].get(prev_class, 0)\n",
    "            \n",
    "            ax.bar(parties, values, label=class_type, bottom=bottom_values, alpha=0.8)\n",
    "        \n",
    "        ax.set_ylabel('Number of Tweets')\n",
    "        ax.set_title('Party Classification Distribution')\n",
    "        ax.legend()\n",
    "        ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    def plot_sentiment_comparison(self, data_dict: Dict[str, pd.DataFrame]):\n",
    "        \"\"\"Create comprehensive visualization comparing sentiment across parties\"\"\"\n",
    "        try:\n",
    "            fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "            fig.suptitle('Australian Political Sentiment Analysis', fontsize=16, fontweight='bold')\n",
    "            \n",
    "            # Prepare data for plotting\n",
    "            parties = list(data_dict.keys())\n",
    "            colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A']  # Expandable color palette\n",
    "            \n",
    "            # 1. Sentiment Distribution Comparison\n",
    "            self._plot_sentiment_distribution(axes[0, 0], data_dict, parties, colors)\n",
    "            \n",
    "            # 2. Average Sentiment Scores\n",
    "            self._plot_average_sentiment(axes[0, 1], data_dict, parties, colors)\n",
    "            \n",
    "            # 3. Engagement Metrics\n",
    "            self._plot_engagement_metrics(axes[1, 0], data_dict, parties, colors)\n",
    "            \n",
    "            # 4. Party Classification Distribution\n",
    "            self._plot_party_classification(axes[1, 1], data_dict, parties)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            self.logger.info(\"Sentiment comparison visualization created successfully\")\n",
    "            return fig\n",
    "        \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error creating visualization: {e}\")\n",
    "            raise DataProcessingError(f\"Visualization failed: {e}\")\n",
    "    \n",
    "    def print_analysis_summary(self, data_dict: Dict[str, pd.DataFrame], summaries: Dict[str, Dict[str, Any]]):\n",
    "        \"\"\"Print comprehensive analysis summary\"\"\"\n",
    "        try:\n",
    "            print(\"=\" * 80)\n",
    "            print(\"üá¶üá∫ AUSTRALIAN POLITICAL SENTIMENT ANALYSIS REPORT\")\n",
    "            print(\"=\" * 80)\n",
    "            print(f\"üìÖ Analysis Period: Last {self.config.DAYS_BACK} days\")\n",
    "            print(f\"üîç Search Language: {self.config.LANG}\")\n",
    "            print(f\"üìä Max Tweets per Query: {self.config.MAX_TWEETS_PER_QUERY}\")\n",
    "            print()\n",
    "            \n",
    "            total_tweets = sum(len(df) for df in data_dict.values() if not df.empty)\n",
    "            print(f\"üìà Total Tweets Analyzed: {total_tweets}\")\n",
    "            print()\n",
    "            \n",
    "            for party, summary in summaries.items():\n",
    "                if 'error' in summary:\n",
    "                    print(f\"‚ö†Ô∏è  {party.upper()} PARTY: {summary['error']}\")\n",
    "                    continue\n",
    "                \n",
    "                print(f\"üèõÔ∏è  {party.upper()} PARTY ANALYSIS\")\n",
    "                print(\"-\" * 50)\n",
    "                print(f\"Total Tweets: {summary['total_tweets']}\")\n",
    "                print(f\"Average Sentiment Score: {summary['avg_compound_score']:.3f}\")\n",
    "                print()\n",
    "                \n",
    "                # Sentiment Distribution\n",
    "                if summary.get('sentiment_distribution'):\n",
    "                    print(\"Sentiment Distribution:\")\n",
    "                    for sentiment, count in summary['sentiment_distribution'].items():\n",
    "                        percentage = (count / summary['total_tweets']) * 100\n",
    "                        print(f\"  ‚Ä¢ {sentiment}: {count} tweets ({percentage:.1f}%)\")\n",
    "                    print()\n",
    "                \n",
    "                # Party Classification\n",
    "                if summary.get('party_classification'):\n",
    "                    print(\"Party Classification:\")\n",
    "                    for classification, count in summary['party_classification'].items():\n",
    "                        percentage = (count / summary['total_tweets']) * 100\n",
    "                        print(f\"  ‚Ä¢ {classification}: {count} tweets ({percentage:.1f}%)\")\n",
    "                    print()\n",
    "                \n",
    "                # Average Engagement\n",
    "                if summary.get('avg_engagement'):\n",
    "                    print(\"Average Engagement:\")\n",
    "                    for metric, value in summary['avg_engagement'].items():\n",
    "                        if pd.notna(value):\n",
    "                            print(f\"  ‚Ä¢ {metric.replace('_', ' ').title()}: {value:.1f}\")\n",
    "                    print()\n",
    "            \n",
    "            # Overall comparison\n",
    "            self._print_comparative_analysis(summaries)\n",
    "            \n",
    "            print(\"=\" * 80)\n",
    "            self.logger.info(\"Analysis summary printed successfully\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error printing summary: {e}\")\n",
    "            print(f\"‚ùå Error generating summary: {e}\")\n",
    "    \n",
    "    def _print_comparative_analysis(self, summaries: Dict[str, Dict[str, Any]]):\n",
    "        \"\"\"Print comparative analysis between parties\"\"\"\n",
    "        valid_summaries = {k: v for k, v in summaries.items() if 'error' not in v}\n",
    "        \n",
    "        if len(valid_summaries) >= 2:\n",
    "            parties = list(valid_summaries.keys())\n",
    "            scores = [valid_summaries[party]['avg_compound_score'] for party in parties]\n",
    "            \n",
    "            print(\"üìä COMPARATIVE ANALYSIS\")\n",
    "            print(\"-\" * 50)\n",
    "            \n",
    "            # Find party with highest sentiment\n",
    "            max_score_idx = scores.index(max(scores))\n",
    "            winner = parties[max_score_idx]\n",
    "            diff = max(scores) - min(scores)\n",
    "            \n",
    "            print(f\"üèÜ Most Positive Sentiment: {winner}\")\n",
    "            print(f\"üìà Sentiment Difference: {diff:.3f}\")\n",
    "            print()\n",
    "            \n",
    "            # Overall statistics\n",
    "            total_positive = sum(\n",
    "                summary['sentiment_distribution'].get('Positive', 0) \n",
    "                for summary in valid_summaries.values()\n",
    "            )\n",
    "            total_negative = sum(\n",
    "                summary['sentiment_distribution'].get('Negative', 0) \n",
    "                for summary in valid_summaries.values()\n",
    "            )\n",
    "            \n",
    "            print(f\"üü¢ Overall Positive Tweets: {total_positive}\")\n",
    "            print(f\"üî¥ Overall Negative Tweets: {total_negative}\")\n",
    "            \n",
    "            if total_negative > 0:\n",
    "                ratio = total_positive / total_negative\n",
    "                print(f\"üìä Positive/Negative Ratio: {ratio:.2f}\")\n",
    "            else:\n",
    "                print(\"üìä All sentiment is positive!\")\n",
    "\n",
    "# Legacy wrapper functions for backward compatibility\n",
    "def plot_sentiment_comparison(data_dict):\n",
    "    \"\"\"Legacy wrapper function\"\"\"\n",
    "    generator = ReportGenerator(config)\n",
    "    return generator.plot_sentiment_comparison(data_dict)\n",
    "\n",
    "def print_analysis_summary(data_dict, summaries):\n",
    "    \"\"\"Legacy wrapper function\"\"\"\n",
    "    generator = ReportGenerator(config)\n",
    "    generator.print_analysis_summary(data_dict, summaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49g8bopndko",
   "metadata": {},
   "source": [
    "# Main Execution - Run Complete Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gdehuv9f9sq",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FileManager:\n",
    "    \"\"\"Handles file operations and data persistence\"\"\"\n",
    "    \n",
    "    def __init__(self, config: AnalysisConfig):\n",
    "        self.config = config\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "    \n",
    "    def save_results(self, analyzed_data: Dict[str, pd.DataFrame], summaries: Dict[str, Dict[str, Any]]) -> tuple[str, str]:\n",
    "        \"\"\"Save analysis results to files\"\"\"\n",
    "        try:\n",
    "            timestamp = dt.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            \n",
    "            # Save combined data\n",
    "            all_results = [df for df in analyzed_data.values() if not df.empty]\n",
    "            \n",
    "            if all_results:\n",
    "                combined_df = pd.concat(all_results, ignore_index=True)\n",
    "                filename = f\"aus_political_sentiment_{timestamp}.csv\"\n",
    "                combined_df.to_csv(filename, index=False)\n",
    "                self.logger.info(f\"Results saved to: {filename}\")\n",
    "                \n",
    "                # Save summary as JSON\n",
    "                import json\n",
    "                summary_filename = f\"aus_political_summary_{timestamp}.json\"\n",
    "                \n",
    "                # Convert numpy types to native Python types for JSON serialization\n",
    "                json_summaries = {}\n",
    "                for party, summary in summaries.items():\n",
    "                    json_summary = {}\n",
    "                    for key, value in summary.items():\n",
    "                        if isinstance(value, dict):\n",
    "                            json_summary[key] = {k: float(v) if pd.notna(v) else None for k, v in value.items()}\n",
    "                        elif pd.notna(value):\n",
    "                            json_summary[key] = float(value) if isinstance(value, (int, float)) else value\n",
    "                        else:\n",
    "                            json_summary[key] = value\n",
    "                    json_summaries[party] = json_summary\n",
    "                \n",
    "                with open(summary_filename, 'w') as f:\n",
    "                    json.dump(json_summaries, f, indent=2, default=str)\n",
    "                \n",
    "                self.logger.info(f\"Summary saved to: {summary_filename}\")\n",
    "                return filename, summary_filename\n",
    "            else:\n",
    "                self.logger.warning(\"No results to save\")\n",
    "                return \"\", \"\"\n",
    "        \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error saving results: {e}\")\n",
    "            return \"\", \"\"\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function - orchestrates the complete analysis pipeline\"\"\"\n",
    "    try:\n",
    "        logger.info(\"üöÄ Starting Australian Political Sentiment Analysis\")\n",
    "        print(\"üöÄ Starting Australian Political Sentiment Analysis\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Initialize components\n",
    "        orchestrator = AnalysisOrchestrator(client, config)\n",
    "        report_generator = ReportGenerator(config)\n",
    "        file_manager = FileManager(config)\n",
    "        \n",
    "        # Step 1: Run complete analysis\n",
    "        print(\"\\nüß† ANALYSIS PIPELINE\")\n",
    "        print(\"-\" * 30)\n",
    "        analyzed_data, summaries = orchestrator.run_complete_analysis()\n",
    "        \n",
    "        # Check if we have any data\n",
    "        total_collected = sum(len(df) for df in analyzed_data.values() if not df.empty)\n",
    "        if total_collected == 0:\n",
    "            print(\"‚ùå No data collected. Check your API credentials and network connection.\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"\\n‚úÖ Analysis complete! Total tweets processed: {total_collected}\")\n",
    "        \n",
    "        # Step 2: Print comprehensive summary\n",
    "        print(\"\\nüìä ANALYSIS RESULTS\")\n",
    "        print(\"-\" * 30)\n",
    "        report_generator.print_analysis_summary(analyzed_data, summaries)\n",
    "        \n",
    "        # Step 3: Create visualizations\n",
    "        print(\"\\nüìà CREATING VISUALIZATIONS\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        # Only create plots if we have data for at least one party\n",
    "        parties_with_data = [party for party, df in analyzed_data.items() if not df.empty]\n",
    "        if parties_with_data:\n",
    "            try:\n",
    "                fig = report_generator.plot_sentiment_comparison(analyzed_data)\n",
    "                plt.show()\n",
    "                print(\"‚úÖ Visualizations created\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Visualization error: {e}\")\n",
    "                print(f\"‚ö†Ô∏è  Visualization error: {e}\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è  No data available for visualization\")\n",
    "        \n",
    "        # Step 4: Save results\n",
    "        print(\"\\nüíæ SAVING RESULTS\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        csv_file, json_file = file_manager.save_results(analyzed_data, summaries)\n",
    "        if csv_file:\n",
    "            print(f\"‚úÖ Results saved to: {csv_file}\")\n",
    "        if json_file:\n",
    "            print(f\"‚úÖ Summary saved to: {json_file}\")\n",
    "        \n",
    "        if not csv_file and not json_file:\n",
    "            print(\"‚ö†Ô∏è  No results to save\")\n",
    "        \n",
    "        print(\"\\nüéâ Analysis Complete!\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        logger.info(\"Analysis pipeline completed successfully\")\n",
    "        return analyzed_data, summaries\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error during analysis: {str(e)}\"\n",
    "        logger.error(error_msg)\n",
    "        print(f\"\\n‚ùå {error_msg}\")\n",
    "        print(\"Please check your API credentials and try again.\")\n",
    "        return None\n",
    "\n",
    "# Enhanced execution with better error handling\n",
    "if __name__ == \"__main__\":\n",
    "    results = main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}